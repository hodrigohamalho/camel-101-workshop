:walkthrough: Lab 05 - Red Hat OpenShift Streams for Apache Kafka
:codeready-url: {che-url}
:openshift-url: {openshift-host}
:user-password: openshift

= Lab 05 - Red Hat OpenShift Streams for Apache Kafka

On this lab we are going to provision a Kafka cluster using the Red Hat Openshift Streams for Apache Kafka (RHOSAK). 

image::./images/lab-introduction.jpg[]

Once the cluster is provisioned, we are going to use Red Hat Openshift connectors to produce and consume events from the Kafka cluster.

For last, adapt the `Camel quarkus` application to consume and produce JSON events to this cluster.

[time=2]
== Event Driven Architecture Introduction

In an Event Driven Architecture, an event notification is generated, the system captures what happened and waits to provide the response back.  The application that got the notification might either reply right away or wait till the status changes.

image::./images/event-driven-architecture.png[]

Event Driven Architecture allows for more flexible, scalable, contextual, and responsive digital business systems. This is why this architecture style has been gaining popularity.

The key rationale for leveraging Apache Kafka for an Event Driven system is the decoupling of microservices and the development of a Kafka pipeline to connect producers and consumers. Instead of checking for new data, you may just listen to a certain event and take action. Kafka provides a scalable hybrid approach that incorporates both Processing and Messaging.

Another advantage of using Event Driven Architecture with Kafka is that, unlike messaging-oriented systems, events published in Kafka are not removed as soon as they are consumed. They are removed once a specific amount of time has passed.

During their lifetime, they may be read by a variety of consumers, allowing them to respond to a variety of use cases.

[time=5]
== Provisioning a Kafka Cluster in Red Hat Cloud

On the overall lab, now we are going to focus on provisioning the Kafka cluster to enable the applications produce and consume events. 

image::./images/connectors/lab-introduction-kafka-instance.jpg[]

Access the https://developers.redhat.com[Red Hat Developers^] and do the login. In case you do not have registered yet, please do your registration.

image::./images/developer-portal.png[]

{empty} +

Do the Login

image::./images/developer-portal-login.png[]

{empty} +

In case that that you need to do your registration, you should see a form like that:

image::./images/developer-portal-registration.png[]

Access the https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview[Red Hat Streams for Apache Kafka Page^], and click on `Create instance`.

image::./images/stream-for-apache-kafka-page.png[]

{empty} +

If it's your first access, you should see a screen like this:

image::./images/create-kafka-instance.png[]

{empty} +

Access the Learning resources section from the left side menu.

Please take a list of the resources available, there are labs about many technologies that surround the Kafka ecosystem, like:

* *API Designer* - Easily create APIs specs through a rich interface
* *Service Registry* - Manage your schemas
* *Red Hat Openshift Connectors* - Easily integrate third-party systems with the events in Kafka
* *Create applications* using *Quarkus or Node* to access your Kafka instance
* *Databases*

As new technologies are added to the Red Hat Managed Services ecosystem, new learning resources are created in this space. Please let us know in case you miss some information so we can improve this session :-)

image::./images/learning-resources.png[]


{empty} +

Select the `Getting Started with Red Hat Openshift Streams for Apache Kafka` quickstart.

image::./images/learning-resources-2.png[]

{empty} +

[time=20]
== Getting Started with Red Hat Openshift Streams for Apache Kafka

*Follow the lab instructions from the portal*. 
At the end of this lab, you must conclude the following steps:

* Create a Kafka instance
* Copied the connection information
* Create a Service Account 
* Copied the ClientID and Client Secret from the Service Account
* Set the Access Control for the Kafka instance

[type=verification]
Did you manage to achieve the outputs mentioned in the list above?


[time=15]
== Getting started with Red Hat OpenShift Connectors

On this step we are going to create a consumer and a producer to load some information into a Kafka topic. 

image::./images/connectors/lab-introduction-coc.jpg[]

What is Red Hat OpenShift Connectors?

Red Hat OpenShift Connectors are pre-built connectors for quick and reliable connectivity across data, services, and systems.

image::./images/connectors/openshift-connectors-introduction.png[]

* 60+ pre-built source and sink connectors
* Delivered as fully managed service
* User-friendly, code-free user interface
* Tightly integrated with OpenShift Streams for Apache Kafka

{empty} +

Back in the `Learning Resources` session, Select the `Getting started with Red Hat OpenShift Connectors` quickstart.

image::./images/learning-resources-3.png[]

{empty} +

NOTE: The creation of a new topic is not necessary since we can use the topic `my-first-kafka-topic` that was created in the previous lab.

Click on the `Getting started with Red Hat OpenShift Connectors` to open the instructions.

image::./images/connectors/01-learning-resource.png[]

{empty} +

From the side menu, select `Connectors` > `Connectors Instance`. Click on Create a connection instance.

image::./images/connectors/02-connector-instances.png[]

{empty} +

In Connector type `data` in the search box, so select the `Data Generator source`.

image::./images/connectors/03-create-connector.png[]

{empty} +

Select the Kafka instance previously created.

image::./images/connectors/03-create-connector-2.png[]

{empty} +

Click on `Create a preview namespace`.

image::./images/connectors/03-create-connector-3.png[]

{empty} +

Confirm it.

image::./images/connectors/03-create-connector-4.png[]

{empty} +

Select the preview namespace.

image::./images/connectors/03-create-connector-5.png[]

{empty} +

Create the connector instance with the following information in the `Core` session:

* Connector instance name: `kafka-producer-generator`
* Client ID: <previous-created>
* Client Secret: <previous-created>

{empty} +

If you don't remember your Client ID and Client Secret, click in `Create service account` and create a new one.

image::./images/connectors/03-create-connector-6.png[]

{empty} +

In the `connector specific` session, use the topic `my-first-kafka-topic` was created in the previous lab.

Fill the form with the following instructions: 

. Topic name: `my-first-kafka-topic`
. Content Type: `text/plain`
. Message: `Hello World!`
. Period: 10000

image::./images/connectors/03-create-connector-7.png[]

{empty} +

Review the connector information.

image::./images/connectors/03-create-connector-8.png[]

{empty} +

Check if it's deployed correctly.

image::./images/connectors/03-create-connector-9.png[]

{empty} +

Access the `my-first-kafka-topic` Kafka topic in your Kafka instance.

image::./images/connectors/04-access-kafka-topic.png[]

{empty} +

Go to the `Messages` tab. See if the messages are being stored succesfully in Kafka.

image::./images/connectors/04-access-kafka-topic-messages.png[]

{empty} +

Fine, we succesfully created the Source Connector. 
A Kafka Producer, so right now let's create the Kafka Consumer (Sink Connector).

On the Create Connector page, type: `http sink`.

image::./images/connectors/05-create-sink.png[]

{empty} +

Select the Kafka instance previously created.

image::./images/connectors/05-create-sink-1.png[]

{empty} +

Select the namespace already created.

image::./images/connectors/05-create-sink-2.png[]

{empty} +

On the configuration fill with:

* Instance name: kafka-consumer
* Client ID: The client ID generated in the previous lab
* Client Secret: The client ID generated in the previous lab

image::./images/connectors/05-create-sink-3.png[]

{empty} +

Access the https://webhook.site to get your URL webhook.

image::./images/connectors/05-create-sink-4.png[]

{empty} +

Fill the form with: 

* Consumes Format: `application/octet-stream`
* Method: `POST`
* URL: URL FROM webhook.site
* Topic name: `my-first-kafka-topic`

image::./images/connectors/05-create-sink-5.png[]

{empty} +

In the Error Handling option select: `log`.

image::./images/connectors/05-create-sink-6.png[]

{empty} +

Check if everything is correctly deployed.

image::./images/connectors/05-create-sink-7.png[]

{empty} +

Check if the messages are being generated in the webbook tab from your browser.

image::./images/connectors/06-check-messages-webhook.png[]

{empty} +

Know that we are producing and consuming information from Kafka, let's take a look in the metrics from the Kafka instance dashboard.

image::./images/connectors/07-kafka-metrics.png[]

{empty} +

image::./images/connectors/07-kafka-metrics-1.png[]

== Camel + Quarkus + Kafka

Now, that we explore the Kafka ecosystem. We understood how to properly:

* Create a Kafka cluster in cloud.redhat.com
* Setup Access Control to this cluster.
* Create topic
* Create a consumer/producer using Red Hat Openshift Connectors
* Look at Kafka dashboard metrics

We are going to keep evolving the Camel Quarkus application that we have been working, to integrate with this Kafka Cluster.

image::./images/connectors/lab-introduction-camel-quarkus.jpg[]

